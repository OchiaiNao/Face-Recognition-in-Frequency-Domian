{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import embedding, nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from preprocessing import *\n",
    "# train_data: tuple of length 320, each is a matrix represents a picture\n",
    "# train_target: tuple of length 320, each is a label\n",
    "# For training, there are 40 classes, each has 8 pictures\n",
    "# For testing, there are 40 classes, each has 2 pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = torch.tensor(train_data)\n",
    "train_target = torch.tensor(train_target)\n",
    "test_data = torch.tensor(test_data)\n",
    "test_target = torch.tensor(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the dataset is small, we assumed batch size = 1\n",
    "train_data = train_data.reshape(280,1,1,64,64)\n",
    "test_data = test_data.reshape(120,1,1,64,64)\n",
    "\n",
    "train_target = train_target.reshape(len(train_target), 1)\n",
    "test_target = test_target.reshape(len(test_target), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FaceRecognizer(nn.Module):\n",
    "    def __init__(self, input_dim=64, output_dim=40, batch_size = 1):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)    \n",
    "        self.fc1 = nn.Linear(8*16*16, 512)\n",
    "        self.fc2 = nn.Linear(512, output_dim)\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        step1 = F.relu(self.conv1(input_batch)) \n",
    "        step2 = self.pool(step1)\n",
    "        step3 = F.relu(self.conv2(step2)) \n",
    "        step4 = self.pool(step3).reshape(self.batch_size, -1)\n",
    "        step5 = F.relu(self.fc1(step4))\n",
    "        step6 = self.fc2(step5)\n",
    "        ret = torch.softmax(step6, -1)\n",
    "        \n",
    "        return ret\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0269, 0.0236, 0.0252, 0.0247, 0.0254, 0.0256, 0.0247, 0.0257, 0.0258,\n",
       "         0.0231, 0.0250, 0.0236, 0.0258, 0.0259, 0.0253, 0.0234, 0.0247, 0.0252,\n",
       "         0.0247, 0.0232, 0.0246, 0.0253, 0.0252, 0.0256, 0.0245, 0.0246, 0.0255,\n",
       "         0.0263, 0.0263, 0.0240, 0.0250, 0.0253, 0.0237, 0.0269, 0.0244, 0.0253,\n",
       "         0.0245, 0.0251, 0.0250, 0.0254]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr = FaceRecognizer()\n",
    "fr.forward(train_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(fr.parameters())\n",
    "criterion = nn.CrossEntropyLoss(reduction = 'sum')\n",
    "training_losses = []\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "num_epochs_train = 0\n",
    "\n",
    "def train(model, data, targets, optimizer, criterion, clip, num_epochs=0):  \n",
    "    global num_epochs_train \n",
    "#     if num_epochs_train == 1:\n",
    "#         tmp = optimizer.state_dict()\n",
    "#         tmp[\"param_groups\"][0][\"lr\"] = 0.0005\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    sampling = list(range(train_data.shape[0]))\n",
    "    random.shuffle(sampling)\n",
    "    print(\"training ...\")\n",
    "    for i, selected_batch_index in tqdm(enumerate(sampling)):\n",
    "        optimizer.zero_grad()\n",
    "        z = fr.forward(data[selected_batch_index])\n",
    "        loss = 0       \n",
    "        loss=criterion(z,targets[selected_batch_index].long())\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    num_epochs_train += 1\n",
    "    return epoch_loss\n",
    "\n",
    "confusion_matrix = []\n",
    "num_epochs = 0\n",
    "def evaluate(model, data, targets, criterion, num_targets): \n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    sampling = list(range(data.shape[0]))\n",
    "    random.shuffle(sampling)\n",
    "    confusion_matrix.append(torch.zeros(num_targets,num_targets))\n",
    "    global num_epochs\n",
    "    for i, selected_batch_index in tqdm(enumerate(sampling)):\n",
    "        z = fr.forward(data[selected_batch_index])\n",
    "        loss = 0\n",
    "        loss=criterion(z,targets[selected_batch_index].long())\n",
    "        epoch_loss += loss.item()\n",
    "        # Load in confusion_matrix\n",
    "        for i in range(data[selected_batch_index].shape[0]):\n",
    "            row = targets[selected_batch_index].long()\n",
    "            col = torch.argmax(z[i])\n",
    "            \n",
    "#             print(num_epochs,row.item(),col.item())\n",
    "#             print(confusion_matrix)\n",
    "            confusion_matrix[num_epochs][row.item()][col.item()] += 1\n",
    "     \n",
    "    num_epochs += 1\n",
    "        \n",
    "    return epoch_loss\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch start:  0\n",
      "training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "280it [00:01, 146.89it/s]\n",
      "120it [00:00, 807.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 2s\tTrain Loss: 765.664 | Test Loss: 329.758\n",
      "epoch start:  1\n",
      "training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "280it [00:01, 147.13it/s]\n",
      "120it [00:00, 1122.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Time: 0m 2s\tTrain Loss: 765.664 | Test Loss: 329.743\n",
      "epoch start:  2\n",
      "training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "280it [00:01, 153.53it/s]\n",
      "120it [00:00, 1081.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Time: 0m 1s\tTrain Loss: 765.664 | Test Loss: 329.744\n",
      "epoch start:  3\n",
      "training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "280it [00:01, 149.48it/s]\n",
      "120it [00:00, 1154.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Time: 0m 1s\tTrain Loss: 765.663 | Test Loss: 329.755\n",
      "epoch start:  4\n",
      "training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "280it [00:01, 149.85it/s]\n",
      "120it [00:00, 1140.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Time: 0m 1s\tTrain Loss: 765.663 | Test Loss: 329.692\n"
     ]
    }
   ],
   "source": [
    "\n",
    "N_EPOCHS = 5\n",
    "CLIP = 1\n",
    "best_test_loss = 999999\n",
    "for epoch in range(N_EPOCHS):  \n",
    "    print(\"epoch start: \", epoch)  \n",
    "    start_time = time.time()\n",
    "    training_loss = train(fr, train_data, train_target, optimizer, criterion, CLIP)\n",
    "    training_losses.append(training_loss)\n",
    "    test_loss = evaluate(fr, test_data, test_target, criterion, 40)\n",
    "    test_losses.append(test_loss)  \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss \n",
    "        torch.save(fr.state_dict(), 'best_model.pt')\n",
    "        \n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s', end='')\n",
    "    print(f'\\tTrain Loss: {training_loss:.3f} | Test Loss: {test_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 3., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 2.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 3., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 3., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 2.]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 3., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 3.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 3., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 3., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 3.]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(119.)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum = 0\n",
    "matrix = -1\n",
    "for i in range(confusion_matrix[matrix].shape[0]):\n",
    "    sum = sum + confusion_matrix[matrix][i][i]\n",
    "sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
